# Домашнее задание №1: Оптимизация градиентным спуском

## Описание
Реализация алгоритма градиентного спуска с моментом для оптимизации функции Химмельблау. Включает как консольную версию для решения задач, так и Jupyter notebook для изучения и визуализации.

## Требования
- Python 3.7+
- NumPy
- Matplotlib (для notebook)
- Jupyter (для notebook)

## Установка зависимостей
```bash
pip install -r requirements.txt
```

## Структура файлов
```
Homework№1-Gradient_descent_optimization/
├── himmelbau.py                                    # Консольная версия
├── Workshop__Gradient_descent_optimization.ipynb  # Jupyter notebook
├── requirements.txt                               # Зависимости
└── README.md                                     # Этот файл
```

## Запуск консольной версии
```bash
python himmelbau.py
```

### Входные данные
Программа ожидает две координаты (x, y) через пробел:
```
5.0 5.0
```

### Выходные данные
Координаты найденного минимума:
```
3.5844283403300017 1.8481265269644037
```

## Запуск Jupyter notebook
```bash
jupyter notebook Workshop__Gradient_descent_optimization.ipynb
```

## Функция Химмельблау
```
f(x₁, x₂) = (x₁² + x₂ - 11)² + (x₁ + x₂² - 7)²
```

Функция имеет 4 глобальных минимума:
- (3, 2)
- (-2.805118, 3.131312)
- (-3.779310, -3.283186)
- (3.584428, -1.848126)

## Алгоритм
Используется градиентный спуск с моментом:
- Скорость обучения: α = 0.0025
- Момент: β = 0.8
- Толерантность: 1e-15
- Максимум итераций: 500
- Ограничения: [-10, 10] для обеих переменных

## Пример использования
```bash
$ python himmelbau.py
5.0 5.0
3.5844283403300017 1.8481265269644037
```

## Особенности реализации
- Градиенты вычисляются аналитически
- Используется момент для ускорения сходимости
- Применяются ограничения для предотвращения расхождения
- Корректная обработка граничных случаев

## Теория
Градиентный спуск с моментом:
```
vₜ = βvₜ₋₁ + (1-β)∇f(xₜ₋₁)
xₜ = xₜ₋₁ - αvₜ
```

Где:
- vₜ - скорость на шаге t
- β - коэффициент момента
- α - скорость обучения
- ∇f(x) - градиент функции
